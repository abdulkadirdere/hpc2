
\begin{document}
\title{COMS4040A \& COMS7045A Assignment 3 -- Report}
\author{Abdulkadir Dere - 752817 - Computer Science Hons}
\date{29 May 2020} 
\maketitle 
%\thispagestyle{empty}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\fancyhead[L]{COMS4040A \& COMS7045A Assignment 3}
%\vskip 3mm 
%\pagenumbering{roman}
%\newpage



\lstset{language=C,%
    basicstyle=\ttfamily,
    frame=single,
    breaklines=true,%
    % belowskip=3em,
    morekeywords={matlab2tikz},
    keywordstyle=\color{blue},%
    morekeywords=[2]{1}, keywordstyle=[2]{\color{black}},
    identifierstyle=\color{black},%
    stringstyle=\color{mylilas},
    commentstyle=\color{mygreen},%
    showstringspaces=false,%without this there will be a symbol in the places where there is a space
    numbers=left,%
    numberstyle={\tiny \color{black}},% size of the numbers
    numbersep=9pt, % this defines how far the numbers are from the text
    % emph=[1]{for,end,break},emphstyle=[1]\color{red}, %some words to emphasise
    emph=[1]{serial_convolution, global_convolution, shared_convolution, convolution, applyMask, neighbours, },emphstyle=[1]\color{red}, %some words to emphasise
    %emph=[2]{word1,word2}, emphstyle=[2]{style},    
}


\section{Introduction} 
In this report, we will focus on the different design methodologies used to create 2D convolution on provided images. We will look at serial implementation, CUDA implementation using both global memory and constant memory, CUDA implementation using both shared memory and constant memory and CUDA implementation using texture memory. Each design methodology is defined and the process has been explained with the code.

\section{Methodology}
We will be using averaging, edge detection and sharpening masks for the convolution process.
\begin{table}[H]
\centering
\begin{tabular}{|l|l|l|}
\hline
-1 & -1 & -1 \\ \hline
-1 & 9  & -1 \\ \hline
-1 & -1 & -1 \\ \hline
\end{tabular}
\hspace{0.5cm}
\begin{tabular}{|l|l|l|}
\hline
-1 & 0 & 1 \\ \hline
-2 & 0  & 2 \\ \hline
-1 & 0 & 1 \\ \hline
\end{tabular}
\hspace{0.5cm}
\begin{tabular}{|l|l|l|}
\hline
1 & 1 & 1 \\ \hline
1 & 1  & 1 \\ \hline
1 & 1 & 1 \\ \hline
\end{tabular}
\caption{Masks used for convolution: sharpening, edge detection and averaging respectively}
\label{tab:my-table}
\end{table}

\subsection{Serial Computation}
We will first implement the serial version of 2D image convolution algorithm using the CPU. Serial version of the algorithm will follow the following design methodology:
\begin{itemize}[leftmargin=0em]
%\setlength\itemsep{-0.2em}
  \setlength{\itemsep}{0pt}
  \item Read the image using built-in functions. 2D image is returned as 1D image and assigned to a variable.
  \item Convert the read image from 1D to 2D for easier computation of convolution algorithm since the image is in 2D.
  \item Pad the image so mask we can compute the corners of the image. The image will be padded with zero padding. 
  \item Apply the convolution mask/filter to the padded image. The convolution algorithm will be executed as a recursive algorithm. The process will start by selecting the first element of the array excluding the padded pixels. We will subtract padded size from the offset (half size of the mask) to find the starting row and column of the image. This pixel must be the same as the original pixel. We will apply the convolution mask to this pixel. (Algorithm \ref{lst: serial_convolution})
  \item The convolution mask application process starts by identifying the neighbours of the given pixel location. We will compute neighbours according to the mask size since the image will be padded by the factor of half of the mask on all four sides. We will not run out-of-bounds when retrieving the pixel values for each neighbour since the image is padded. We will create a convolution results array with the dimensions of the mask size to compute the results. We will do element-wise multiplication between neighbours and the mask. All of these values will be added together to compute the convolution result of the given pixel location. (Algorithm \ref{lst: applyMask})
  \item The convolution result for the pixel location will be saved in the output image and process will continue with the next pixel location until the algorithm iterates through all the pixels. (Algorithm \ref{lst: serial_convolution})
  \item The padded area will be removed by un-padding the padded image to retrieve the results in original image dimensions.
  \item The resulting image will be converted from 2D array to 1D to save the resulting convolution image. 
\end{itemize}

The code for the detailed methodology has been provided with comments. Pre-processing and post-processing functions are not included in the report. These functions can be founds in the following file: $serialConvolution.cu$ under $src$ folder.

\begin{lstlisting}[language=C, label={lst: serial_convolution}, caption= Add zero padding to the image]
// 2D serial convolution method
double **serial_convolution(double **input, double **output){
    int range = padded_size - offset;
    // printf("range: %d \n", range);

    for (int i = offset; i<range; i++){
        for (int j = offset; j<range; j++){
            output[i][j] = applyMask(input, i, j);
        }
    }
    return output;
}
\end{lstlisting}

\begin{lstlisting}[language=C, label={lst: applyMask}, caption= Apply convolution mask to the given pixel]
double applyMask(double **array, int row, int col){
    int n_size = offset * 2 + 1;

    // neighbours of given location
    double **neighbours = allocateMatrix(n_size, n_size);

    // dynamically get the neighbours range
    int n1 = 0;
    for (int r=row - 1; r <= row + offset; r++){
        int n2 = 0;
        for (int c =col - 1; c <= col + offset; c++){
            neighbours[n1][n2] = array[r][c];
            n2++;
        }
        n1++;
    }
    
    double **convolution = allocateMatrix(n_size, n_size);
    double value = 0;
    for (int r=0; r<3; r++){
        for(int c=0; c<3; c++){
            convolution[r][c] = mask[r][c] * neighbours[r][c];
            value = value + convolution[r][c];
        }
    }
    return value;
}
\end{lstlisting}


\subsection{CUDA implementation using both global memory and constant memory}
Image convolution using CUDA C has been implemented using both global memory and constant memory. The convolution mask is constant throughout the convolution process. It is beneficial to cache the convolution mask in the constant memory as this informs the CUDA runtime that mask values will not change during kernel execution (\cite{parallel}) (Algorithm \ref{lst: mask_global}). The constant memory will set the mask value as read-only and it will be broadcasted to all elements in the convolution kernel execution. The convolution process will be done in global memory using a CUDA kernel (Algorithm \ref{lst: global_convolution}). The kernel parameters consist of original image, allocated space for resulting image, width and height of the original image. The row and column indexes will be computed to identify which index will be computed by which thread. The starting index for row and column will be calculated by subtracting the offset value because we want to ignore the padded area. This will let the convolution process to start and end at dimensions of the original image so we don't run out-of-bounds. The kernel will calculate each convolution value by calculating all the elements within the mask filter size (dimension) and add all of them to get the convolution value for a specific pixel. The row and column indexes will be verified so that we are within the dimensions. The resulting value will be saved to the resulting image space.

These functions can be founds in the following file: $globalConvolution.cu$ under $src$ folder.
\begin{lstlisting}[language=C, label={lst: mask_global}, caption= Cache mask in to the constant memory]
// Convolution Mask Dimension
#define MASK_DIM 3
#define OFFSET (MASK_DIM/2)

// allocate mask in constant memory
__constant__ float d_mask_global[MASK_DIM * MASK_DIM];
\end{lstlisting}


\begin{lstlisting}[language=C, label={lst: global_convolution}, caption= 2D Convolution using the global memory]
// 2D convolution using global and constant memory
__global__ void global_convolution(float *d_Data, float *d_result, int width, int height) {
  // calculate the row and column index to compute for each thread
  int row = blockIdx.y * blockDim.y + threadIdx.y;
  int col = blockIdx.x * blockDim.x + threadIdx.x;

  // Starting index for convolution so we can ignore the padded area
  int i_row = row - OFFSET;
  int i_col = col - OFFSET;

  // convolution value to be calculated for each pixel's row and column
  double value = 0;
  // iterate over all rows and column using the mask dimension.
  // this will calculate all the neighbours and origin pixel and sum these values to give
  // us the value of the origin pixel
  for (int i = 0; i < MASK_DIM; i++) {
    for (int j = 0; j < MASK_DIM; j++) {
      if ((i_row + i) >= 0 && (i_row + i) < height && (i_col + j) >= 0 && (i_col + j) < width) {
      	// sum all the values within the range of the mask to get origin pixel's value
           value += d_Data[(i_row + i) * width + (i_col + j)] * d_mask_global[i * MASK_DIM + j];
      }
    }
  }
  // write back convolution result
  d_result[row * width + col] = value;
}
\end{lstlisting}


\subsection{CUDA implementation using both shared memory and constant memory}
In this section, we will look at image convolution using both shared memory and constant memory. The constant memory is used for caching the masks for read-only purposes as it is required by every element during the convolution process (\ref{lst: mask_shared}). The kernel parameters consist of the original image, the resulting image, width and height of the original image. Block sized tiles are created for shared memory tiling process. Each tile's row and column indexes are computed for the utilisation by threads. We need the boundaries of the tile so we don't run out-of-bounds and each thread works only on its own tile and does not modify other tiles values. Tiles are loaded from global memory to shared memory for faster access to tile data. Thread barrier has been set so all the threads need to finish loading data before the kernel continues with the process. We will iterate through the rows and columns within the mask dimensions and also check if we are still within the tile bounds. The convolution masks will be applied to every element within the tile within the dimensions of the mask. Hence results can be summed to retrieve the output result for a given pixel.

These functions can be founds in the following file: $sharedConvolution.cu$ under $src$ folder.
\begin{lstlisting}[language=C, label={lst: mask_shared}, caption= Cache mask in to the constant memory]
// Convolution Mask Dimension
#define MASK_DIM 3
#define OFFSET (MASK_DIM/2)

#define TILE_WIDTH 16
#define RADIUS 2
#define BLOCK_WIDTH (TILE_WIDTH+(2*RADIUS))

#define DIAMETER (RADIUS*2+1) // mask diameter
#define SIZE (RADIUS*DIAMETER) // mask size

// allocate mask in constant memory
__constant__ float d_mask_shared[MASK_DIM * MASK_DIM];
\end{lstlisting}


\begin{lstlisting}[language=C, label={lst: shared_convolution}, caption= 2D Convolution using the shared memory]
__global__ void shared_convolution(float* dData, float* dResult, unsigned int width, unsigned int height){

  // create tile in shared memrory for the convolution
  __shared__ float shared[BLOCK_WIDTH * BLOCK_WIDTH];

    // for simplicity to use threadIdx
    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int bx = blockIdx.x;
    int by = blockIdx.y;

    // get row and column index of pixels in the tile
    int col = bx * TILE_WIDTH + tx - RADIUS;
    int row = by * TILE_WIDTH + ty - RADIUS;

    // Find the last and first pixel locations within the image
    col = max(0, col);
    col = min(col, width-1);
    row = max(row, 0);
    row = min(row, height-1);

    // load the tile pixels from the global memory into shared memory
    // this will help us to reduce global memory access by the factor of 1/TILE_WIDTH
    // ignore any pixels which are out-of-bounds (i.e. padded area)
    unsigned int index = row * width + col;
    unsigned int block_index = ty * blockDim.y + tx;
    shared[block_index] = dData[index];

    // thread barrier to wait for all the threads to finish loading from
    // global memory to shared memory
    __syncthreads();
  
    // Elementwise multiplication of pixel and mask values and add all of the values within the mask
    // range to get output value of one pixel. Verify that we are not working out-of-bounds of the image
    // We will iterate over rows and columns within the mask dimensions (i.e. all the neighbours)
    float value = 0;
    if (((tx >= RADIUS) && (tx < BLOCK_WIDTH-RADIUS)) && ((ty>=RADIUS) && (ty<=BLOCK_WIDTH-RADIUS))){
      for(int i = 0; i<MASK_DIM; i++){
          for(int j = 0; j<MASK_DIM; j++){ 
            value += shared[block_index+(i*blockDim.x)+j] * d_mask_shared[i*3+j];
          }
      }
      dResult[index] = value;
  }
}\end{lstlisting}

The dimensions of the block are set to block width (\ref{lst: dim_shared}). Block dimension is tile width summed with two times the offset. The offset is the padded area of the image. The offset is multiplied by two to cater for the padding of opposite sides of the image (e.g. left and right). Hence, the block dimension is set to block width so all the blocks are padded and pixels at the block corners can be computed. The padded area of each block is ignored in the kernel when are calculating the starting row and columns. The grid dimensions are set to cater for the minimum number of grids required to compute the image convolution for the whole image.
\begin{lstlisting}[language=C, label={lst: dim_shared}, caption= Shared memory kernel dimensions]
#define TILE_WIDTH 16
#define OFFSET 2
#define BLOCK_WIDTH (TILE_WIDTH+(2*OFFSET))
dim3 dimGrid(BLOCKS, BLOCKS);
dim3 dimBlock(BLOCK_WIDTH, BLOCK_WIDTH);
\end{lstlisting}


\section{Experiment} 
\subsection{Experiment Setup}
Experiments are conducted on a cluster. The details for the CUDA device are listed.\\
CUDA Device 0\\
Major revision number:         6\\
Minor revision number:         1\\
Name:                          GeForce GTX 1060 6GB \\ 
Total global memory:           6371475456 \\
Total shared memory per block: 49152 \\
Total registers per block:     65536 \\
Warp size:                     32 \\ 
Maximum memory pitch:          2147483647 \\
Maximum threads per block:     1024 \\
Maximum dimension 0 of block:  1024 \\
Maximum dimension 1 of block:  1024 \\
Maximum dimension 2 of block:  64 \\
Maximum dimension 0 of grid:   2147483647 \\
Maximum dimension 1 of grid:   65535 \\
Maximum dimension 2 of grid:   65535 \\
Clock rate:                    1784500 \\
Total constant memory:         65536 \\
Texture alignment:             512 \\
Concurrent copy and execution: Yes \\
Number of multiprocessors:     10 \\
Kernel execution timeout:      Yes \\

\subsection{Experiment Results}
Experimental results are shown for the serial, global memory and shared memory implementations for the image convolution.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Image}                                                                                           & \textbf{image21.pgm} & \textbf{lena\_bw.pgm} \\ \hline
\textbf{Matrix Size}                                                                                     & 512x512              & 512x512               \\ \hline
\textbf{Tile Size}                                                                                       & 16x16                & 16x16                 \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Serial Convolution\\ Time (ms)\end{tabular}}                          & 154.862244           & 152.380386            \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Global Memory\\ Time (ms)\end{tabular}}                               & 0.093184             & 0.094880              \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Shared Memory \\ Time (ms)\end{tabular}}                              & 0.030656             & 0.032704              \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Speedup of global memory\\ kernel (ms)\end{tabular}}                  & 1661.9               & 1606.03               \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Speedup of shared memory\\ kernel (ms)\end{tabular}}                  & 5051.61              & 4659.38               \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Throughput of serial\\ implementation (GFLOPS)\end{tabular}}          & 0.0152348            & 0.0154829             \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Throughput of global\\ memory implementation\\ (GFLOPS)\end{tabular}} & 25.3187              & 24.8661               \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Throughput of shared\\ memory implementation\\ (GFLOPS)\end{tabular}} & 76.9603              & 72.1409               \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Performance improvement: \\ global over serial\end{tabular}}          & 1661.9x              & 1606.03x              \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Performance improvement: \\ shared over serial\end{tabular}}          & 5051.61x             & 4659.38x              \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Performance improvement: \\ shared over global\end{tabular}}          & 3.03967x             & 2.90117x              \\ \hline
\end{tabular}
\caption{Results of the convolutions applied to the given images.}
\label{tab:results-table}
\end{table}

Experimental results are shown for the serial, global memory and shared memory implementations for the image convolution. The results for the different implementations are the same. Hence, the different image results are produced by the different filters used during the convolution process (figure \ref{fig:image21}). The results for the $lead\_bw$ image is more clear (figure \ref{fig:lena}) and the effects can be visualised better. The different design implementations do not affect the resulting image, as expected.

\begin{figure}[H]
\centering
\subfloat[]{\includegraphics[width=.2\textwidth]{image21.png}} \hfill
\subfloat[]{\includegraphics[width=.2\textwidth]{image21_sharpening.png}} \hfill
\subfloat[]{\includegraphics[width=.2\textwidth]{image21_edge.png}} \hfill
\subfloat[]{\includegraphics[width=.2\textwidth]{image21_averaging.png}} \hfill
\caption{Convolution results for the $image21$ image  (a) Original Image (b) Sharpening mask (c) Edge detection mask (d) Averaging mask}
\label{fig:image21}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[]{\includegraphics[width=.2\textwidth]{lena_bw.png}} \hfill
\subfloat[]{\includegraphics[width=.2\textwidth]{lena_bw_global_sharpening.png}} \hfill
\subfloat[]{\includegraphics[width=.2\textwidth]{lena_bw_shared_edge.png}} \hfill
\subfloat[]{\includegraphics[width=.2\textwidth]{lena_bw_averaging.png}} \hfill
\caption{Convolution results for the $lena\_bw$ image (a) Original Image (b) Sharpening mask (c) Edge detection mask (d) Averaging mask}
\label{fig:lena}
\end{figure}

The resulting images for all the kernels can be found in the $results$ folder.

The averaging filter with 16 elements (4x4 size) results are as follows:\\
Serial Image Convolution Time: 156.733444 ms \\
Global Memory Time elapsed: 0.153600 ms \\
Shared Memory Time elapsed: 0.034880 ms \\

We can see that the global memory version takes almost double the kernel with 3x3 size as shown in table \ref{tab:results-table}. However, the shared memory version still performs similar results compared to the smaller sized averaging filter. 

\subsection{Summary of Results}
As we can see from table~\ref{tab:results-table} that global memory version has greatly improved on the serial implementation. The shared memory version has improved on global memory and serial implementations. However, global memory implementation is still not implemented efficiently since every thread calculates its own output. This results in global memory access for each thread. The global memory version can be improved by implementing threads to compute a tile of the image similar to shared memory implementation. This will decrease access to global memory. 


\section{Questions and Answers}
(a) How many floating point operations are performed in your convolution kernel using global memory?
The floating point operations are calculated by multiplying image dimensions with mask dimensions. Hence, \(FLOP = width * height * mask\_width * mask\_height\). All the provided images are the size of 512 x 512 and mask size is kept at 3x3 in most cases. Therefore, $FLOP = 512 * 512 * 3 * 3 = 2 359 296$.

(b) How many global memory reads are performed by your kernel using global memory and kernel using shared memory, respectively?
The global memory version performs a global memory read for each thread since every thread calculates its own output. This is inefficient as discussed before. The shared memory version has less memory reads since we load the tiles from global memory to shared memory. The shared memory reads are $\frac{1}{TILE\_WIDTH}$ of the global memory kernel because we are accessing the global memory only to load the tile from global memory to shared memory. Hence, the memory access decreases by the factor of $\frac{1}{TILE\_WIDTH}$ of the global memory.

(c) How many global memory writes are performed by your convolution kernel using shared memory ?
The global memory writes are conducted by each thread. Hence, the global memory write will be 512x512 = 262144 writes to the global memory for an image with the dimensions of 512x512. The threads are restricted in a way that only the threads within the limits of the image can write to the global memory. Hence, we are calculating the number of threads which has write access to the output image.

(d) What would happen to the performance of your kernel using shared and constant memory when the size of the ‘averaging’ mask increases (say, to a substantial large size) ?
The issue is that mask is cached in the constant memory. So any mask size which exceeds the limits of cache memory capacity will not allow the program to compile and run. Hence, it will not work. Another is the kernel dimensions are dependent on the tile width which is also affected by the mask size. Hence, any changes to mask size will break the limitations of the kernel. Hence, CUDA will not be able to compile and run the application.

\section{Conclusion} 
In this report, we have discussed and showcased different image convolution methods using various device memories. The usage of shared memory shows significant performance increases compared to global memory and serial implementations. The usage of constant memory allowed us to access convolution masks as read-only data and sped-up the process.  These can be visualised in the given tables and results. 

\newpage
\bibliographystyle{apalike}
\bibliography{bibfile}

\pagenumbering{arabic} 
\end{document} 

