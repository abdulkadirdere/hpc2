
\begin{document}
\title{COMS4040A \& COMS7045A Assignment 2 -- Report}
\author{Abdulkadir Dere - 752817 - Computer Science Hons}
\date{14 May 2020} 
\maketitle 
%\thispagestyle{empty}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\fancyhead[L]{COMS4040A \& COMS7045A Assignment 2}
%\vskip 3mm 
%\pagenumbering{roman}
%\newpage
\section{Introduction} 

\section{Methodology} 
\subsection{Matrix Transpose}

\subsection{Vector Addition}

\subsection{Tiled Matrix Multiplication}

\section{Experiment} 
\subsection{Experiment Setup}


\subsection{Experiment Data}
Experiments are done using synthetic data. Synthetic data is generated by using a random number generator algorithm for the reference and query matrices. Following sizes have been used for the variables $m$, $n$ and $d$ in different experiments as shown in results. We have used 100000 for $m$ references and 10 for $d$ dimensions. We have altered $q$ query points between 100 and 200.

\subsection{Experiment Results}
Experimental results are shown with different distance and sorting algorithms and with varying $n$ query points.


\subsection{Summary of Results}
Experiment results indicate that parallelising the sequential distance and sorting algorithms improve the computation time. The combination of using manhattan distance and merge sort results in the most efficient computation (table:\ref{tab:parallelManhattanMergeTask}) followed by euclidean distance and merge sort (table:\ref{tab:parallelEuclideanMerge}) combination. This indicates that on large sets of data, merge sort performs better than quick sort. Comparison between using task construct (table:\ref{tab:parallelManhattanMergeTask}) and section construct (table:\ref{tab:parallelManhattanMergeSection}) shows that task construct performs significantly better than the sections construct in the parallel program. Results of parallelising selection sort (table:\ref{tab:parallelEuclideanSelection}) shows that any sorting algorithm which does not adopt the divide and conquer approach performs poorly compared to the quick and merge sort algorithms, which do use divide and conquer approach. Euclidean distance calculation performs better when compared to manhattan distance calculation as can be seen in tables \ref{tab:parallelEuclideanQuick} and \ref{tab:parallelManhattanQuick} .

\section{Conclusion} 
We have implemented $k$NN algorithm in both sequential and parallel methods. We have focused on parallelising the distance and sorting algorithms for the $k$NN algorithm. We have conducted experiments with different distance and sorting functions and recorded the results of the experiments. As hypothesised, parallelising the distance and sorting algorithms improved the computation time of $k$NN algorithm significantly. Divide and conquer type of sorting algorithms perform computationally better when parallelised compared to other sorting algorithms. 

In order to get the best results it is important to use divide and conquer approach algorithms,  because of their exceptionally good computational performance when computed in parallel. Using the correct sorting algorithm is key to the computation performance since sorting occupies most of the computation time as can be seen in the experiment result percentages. Implemented methods are sufficient enough to assess the importance of parallelising distance and sorting algorithms.

\newpage
\bibliographystyle{apalike}
\bibliography{bibfile}

\pagenumbering{arabic} 
\end{document} 

